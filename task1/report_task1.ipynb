{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Systems Task 1\n",
    "_Daniel Jones, Praful Chunchu, Ravi Patel and Austin Staton_\n",
    "\n",
    "**Objective**: Generating adversarial attacks in the context of a zero-knowledge threat model.\n",
    "\n",
    "We will be exploring various adversarial attacks, including: _Projected Gradient Descent_, _Fast Gradient Sign Method_, and the _Basic Iterative Method_. We will be generating these adversarial examples with a specific set of tuned parameters for each attack, to also allow for a demonstration of the effectiveness of different machine learning models.\n",
    " \n",
    "### Experimental Design\n",
    "We will be attacking three different models: an undefended model, the [vanilla Athena](https://github.com/softsys4ai/athena) with a manually selected ensemble of 5 weak defenses, and PGD-ADT. \n",
    "\n",
    "In order to effectively determine the differences in their success (or rather, their differences in errors) between each different approach, identical parameters will be sent to each different model, respectively by adversarial attack. Meaning, for any _one_ attack, (PGD, FGSM, BIM) the parameters testing the attack's efficacy will remain constistent across the three differently independant models. \n",
    "\n",
    "All  generated adversarial examples will utilize the same epsilon values of `0.03`, `0.06`, `0.12`, and `0.48`. This will allow us to measure and analyze the effectiveness of our adversarial examples in relation to the other AE's we generated. In the case of BIM attacks, each epsilon used will be performed at 50 iterations and 100 iterations to explore the impact iterations can have on the error rate generated by an adversarial example.\n",
    "\n",
    "We expect this to give some experimental consistency to our results.\n",
    "\n",
    "\n",
    "#### Subsampling\n",
    "To increase execution speed, we opted to generate subsamples of data using the provided `subsample.py` script provided.This sctipt generated subsamples at a ratio of `0.1` and the subsamples/sublabels used for this experiment can be found in the `/task1/SubSample` folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Relevant Files\n",
    "All of the adversarial examples generated with `craft_adversarial_examples.py` are located in the `/task1/Adversarial_Examples` folder. There are three different attack methods, each with 5 different epsilon values, named according to their variant. BIM has 10 attack methods due to running them at 50 and 100 iterations. This totals to 20 adversarial examples generated as part of this experiment.\n",
    "\n",
    "In `attack-zk-mnist.json` located in `/task1/configs`, there are all of our configurations used to generate adversarial examples.\n",
    "\n",
    "The subsamples and sublabels used during this experiment are located in the `task1/SubSamples` folder and are named `sublabels-mnist-ratio_0.1-112490.080191753.npy` and `subsamples-mnist-ratio_0.1-112490.080191753.npy`.\n",
    "\n",
    "The active weak defenses chosen for the Ensemble are located in `/task1/configs/athena-mnist.json` in the `active_wds` JSON node.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Projected Gradient Descent (PGD)\n",
    "PGD attacks are white-box attacks, specifically designed to take advantage of each layer's weight in the ML model. This attack has a parameter, `epsilon`, that attempts to find the biggest weaknesses in the model, while trying to minimize the input distortion or alteration. We exectued the PGD attack with five different values of `epsilon`.\n",
    "\n",
    "#### The Inputs\n",
    "The parameters of epsilon for the attacks are `0.03`, `0.06`, `0.12`, `0.24`, and `0.48`. When we increase epsilon, two things will happen. The first, is that the inputs (images) will be increasingly poised to exploit the model's weights. The second, which occurs as an effect of the first, is the image's increasing distortion. This is a form of constrained optimization problem that would need to be tuned to each attack's purpose. \n",
    "\n",
    "As an example, if one was attempting to bypass the content filtering of an image upload service, the image would need to be _mostly_ recoverable. Bypassing a content filter to upload an unrecognizable image would not make sense in practical applications.\n",
    "\n",
    "The inputs, in JSON form, looked like the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configs0': {'attack': 'pgd', 'description': 'pgd_eps003', 'eps': 0.03},\n",
       " 'configs1': {'attack': 'pgd', 'description': 'pgd_eps006', 'eps': 0.06},\n",
       " 'configs2': {'attack': 'pgd', 'description': 'pgd_eps012', 'eps': 0.12},\n",
       " 'configs3': {'attack': 'pgd', 'description': 'pgd_eps024', 'eps': 0.24},\n",
       " 'configs4': {'attack': 'pgd', 'description': 'pgd_eps048', 'eps': 0.48}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "\"configs0\": {\n",
    "    \"attack\": \"pgd\",\n",
    "    \"description\": \"pgd_eps003\",\n",
    "    \"eps\": 0.03\n",
    "  },\n",
    "  \"configs1\": {\n",
    "    \"attack\": \"pgd\",\n",
    "    \"description\": \"pgd_eps006\",\n",
    "    \"eps\": 0.06\n",
    "  },\n",
    "  \"configs2\": {\n",
    "    \"attack\": \"pgd\",\n",
    "    \"description\": \"pgd_eps012\",\n",
    "    \"eps\": 0.12\n",
    "  },\n",
    "  \"configs3\": {\n",
    "    \"attack\": \"pgd\",\n",
    "    \"description\": \"pgd_eps024\",\n",
    "    \"eps\": 0.24\n",
    "  },\n",
    "  \"configs4\": {\n",
    "    \"attack\": \"pgd\",\n",
    "    \"description\": \"pgd_eps048\",\n",
    "    \"eps\": 0.48\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Examples\n",
    "The results matched our hypothsis. As the value of the tuned parameter `epsilon` increased, more distortion was created in the image, more model weights were exploited, and more errors occured.\n",
    " \n",
    " \n",
    "**Images at Various Epsilons**\n",
    "\n",
    "As seen below, when the value of epsilon increases, the recognizability of the image decreases; while, the error rate of the classifier increases.\n",
    " \n",
    "In the title of each image, the `X->Y` denotes the classifiers interpretation of the number. `X` represents the original value of the image; `Y` represents its classification by the model after pertubation.\n",
    "\n",
    "0.03            |  0.06 | 0.12 | 0.24 | 0.48\n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|\n",
    "![](images/pgd_eps003.png)  |  ![](images/pgd_eps006.png) |  ![](images/pgd_eps012.png) |  ![](images/pgd_024.png) |  ![](images/pgd_eps048.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Evaluated Models\n",
    "| Adversarial Example | UM         | Ensemble   | PGD-ADT    |\n",
    "|---------------------|------------|------------|------------|\n",
    "| PGD_eps_0.03        | 0.04137235 | 0.00201816 | 0.00605449 |\n",
    "| PGD_eps_0.06        | 0.20282543 | 0.00302725 | 0.01210898 |\n",
    "| PGD_eps_0.12        | 0.84661958 | 0.01009082 | 0.02926337 |\n",
    "| PGD_eps_0.24        | 0.99091826 | 0.03632694 | 0.10191726 |\n",
    "| PGD_eps_0.48        | 0.99091826 | 0.29868819 | 0.64883956 |\n",
    "\n",
    "In this table, its evident that as the value of epsilon increased, the error rate increased as well. This makes sense, since epsilon represents a maginitude of damage to the image. With enough pertubation, one's own mind begins to misclassify an image. It would be expected of a classifier misclassify it too.\n",
    "\n",
    "![PGD Chart](images/PGD_Evaluations.png)\n",
    "\n",
    "As you can see, given our chosen Ensemble of weak defenses for Vanilla Athena, Athena consistantly far outperforms the UM and PGD-ADT models with a lower error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Fast Gradient Signed Method (FGSM)\n",
    "FGSM adversarial attacks are white-box attacks that exploit the gradients, or parameters, to a neural network. It is designed to prioritize speed, rather than designed around solving the constrained optimization problem between data integrity and perturbation, similarly to PGD.\n",
    "\n",
    "FGSM uses the sign of loss function (this is conceptually similar to the linear \"direction\" to the next classification) to determine where the model could easiest misrepresent the data, moves in a \"distance\" of `epsilon` to that next space within the network. \n",
    "\n",
    "With this vector, having a direction (the sign of a loss function) and magnitude (epsilon), can be used to alter input and fool a classifier. \n",
    "\n",
    "#### The Inputs\n",
    "The parameters of epsilon (i.e., distance/magnitude) for the FGSM  attacks are: `0.03`, `0.06`, `0.12`, `0.24`, and `0.48`. In FGSM, `epsilon` is a scalar value that determines how much pertubation to create in the classification.\n",
    "\n",
    "The inputs, in JSON format, looked like the below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configs5': {'attack': 'fgsm', 'description': 'fgsm_eps003', 'eps': 0.03},\n",
       " 'configs6': {'attack': 'fgsm', 'description': 'fgsm_eps006', 'eps': 0.06},\n",
       " 'configs7': {'attack': 'fgsm', 'description': 'fgsm_eps012', 'eps': 0.12},\n",
       " 'configs8': {'attack': 'fgsm', 'description': 'fgsm_eps024', 'eps': 0.24},\n",
       " 'configs9': {'attack': 'fgsm', 'description': 'fgsm_eps048', 'eps': 0.48}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " {\n",
    "  \"configs5\": {\n",
    "    \"attack\": \"fgsm\",\n",
    "    \"description\": \"fgsm_eps003\",\n",
    "    \"eps\": 0.03\n",
    "  },\n",
    "  \"configs6\": {\n",
    "    \"attack\": \"fgsm\",\n",
    "    \"description\": \"fgsm_eps006\",\n",
    "    \"eps\": 0.06\n",
    "  },\n",
    "  \"configs7\": {\n",
    "    \"attack\": \"fgsm\",\n",
    "    \"description\": \"fgsm_eps012\",\n",
    "    \"eps\": 0.12\n",
    "  },\n",
    "  \"configs8\": {\n",
    "    \"attack\": \"fgsm\",\n",
    "    \"description\": \"fgsm_eps024\",\n",
    "    \"eps\": 0.24\n",
    "  },\n",
    "  \"configs9\": {\n",
    "    \"attack\": \"fgsm\",\n",
    "    \"description\": \"fgsm_eps048\",\n",
    "    \"eps\": 0.48\n",
    "  }\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Examples\n",
    "The results matched our hypothsis. When the value of the tuned parameter `epsilon` increased, the 'distance' away from the original classification changed.\n",
    "\n",
    "**Images at Various Epsilons**\n",
    "\n",
    "As seen below, the value of `epsilon` is indirectly proportional to the recognizability of the image; but, the error rate of the classifier is directly proportional.\n",
    "\n",
    "In the title of each image, the `X->Y` denotes the classifiers interpretation of the number. \"X\" represents the original value of the image; \"Y\" represents its classification by the model after pertubation.\n",
    "\n",
    "0.03            |  0.06 | 0.12 | 0.24 | 0.48\n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|\n",
    "![](images/fgsm_eps003.png)  |  ![](images/fgsm_eps006.png) |  ![](images/fgsm_eps012_2.png) |  ![](images/fgsm_eps024.png) |  ![](images/fgsm_eps048.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Evaluated Models\n",
    "| Adersarial Example | UM         | Ensemble   | PGD-ADT    |\n",
    "|--------------------|------------|------------|------------|\n",
    "| FGSM_eps_0.03      | 0.02421796 | 0.00201816 | 0.00605449 |\n",
    "| FGSM_eps_0.06      | 0.09788093 | 0.00201816 |  0.0110999 |\n",
    "| FGSM_eps_0.12      | 0.35822402 | 0.00605449 | 0.02522704 |\n",
    "| FGSM_eps_0.24      | 0.81533804 | 0.03733602 | 0.10292634 |\n",
    "| FGSM_eps_0.48      | 0.91321897 | 0.73360242 | 0.86074672 |\n",
    "\n",
    "In the case of the undefended model, there was a general direct proportionality between epsilon and the error rate of the model. In Vanilla Athena (the ensemble of weak defenses) and PGD-ADT, this trend was not present. This could be attributed to the framework being trained to defend against this adversarial attack.\n",
    "\n",
    "![FGSM Chart](images/FGSM_Evaluations.png)\n",
    "\n",
    "Much like with our results from the Evualation of PGD, Athena with our chosen ensemble consistantly outperformed UM and PGD_ADT.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Basic Iteractive Method (BIM)\n",
    "BIM attacks are a variant to FGSM attacks. The same direction is computed from the loss function and magnitude is found with epsilon; but, the adversarial attack is performed many different times, 'iteratively', in increasing step sizes. \n",
    "\n",
    "#### The Inputs\n",
    "The parameters of epsilon for the attacks are the same as those utilized for PGD and FGSM, however with BIM we ran each epsilon value at `50` iterations and `100` iterations in order to observe the difference in error generated when increasing iteration rates across multiple different epsilon values.\n",
    "\n",
    "The inputs, in JSON form, looked like the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configs10': {'attack': 'bim',\n",
       "  'description': 'bim_eps003iter50',\n",
       "  'eps': 0.03,\n",
       "  'max_iter': 50},\n",
       " 'configs11': {'attack': 'bim',\n",
       "  'description': 'bim_eps006iter50',\n",
       "  'eps': 0.06,\n",
       "  'max_iter': 50},\n",
       " 'configs12': {'attack': 'bim',\n",
       "  'description': 'bim_eps012ter50',\n",
       "  'eps': 0.12,\n",
       "  'max_iter': 50},\n",
       " 'configs13': {'attack': 'bim',\n",
       "  'description': 'bim_eps024iter50',\n",
       "  'eps': 0.24,\n",
       "  'max_iter': 50},\n",
       " 'configs14': {'attack': 'bim',\n",
       "  'description': 'bim_eps048iter50',\n",
       "  'eps': 0.48,\n",
       "  'max_iter': 50},\n",
       " 'configs15': {'attack': 'bim',\n",
       "  'description': 'bim_eps003iter100',\n",
       "  'eps': 0.03,\n",
       "  'max_iter': 100},\n",
       " 'configs16': {'attack': 'bim',\n",
       "  'description': 'bim_eps006iter100',\n",
       "  'eps': 0.06,\n",
       "  'max_iter': 100},\n",
       " 'configs17': {'attack': 'bim',\n",
       "  'description': 'bim_eps0121ter100',\n",
       "  'eps': 0.12,\n",
       "  'max_iter': 100},\n",
       " 'configs18': {'attack': 'bim',\n",
       "  'description': 'bim_eps024iter100',\n",
       "  'eps': 0.24,\n",
       "  'max_iter': 100},\n",
       " 'configs19': {'attack': 'bim',\n",
       "  'description': 'bim_eps048iter100',\n",
       "  'eps': 0.48,\n",
       "  'max_iter': 100}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  {\n",
    "  \"configs10\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps003iter50\",\n",
    "    \"eps\": 0.03,\n",
    "    \"max_iter\": 50\n",
    "  },\n",
    "  \"configs11\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps006iter50\",\n",
    "    \"eps\": 0.06,\n",
    "    \"max_iter\": 50\n",
    "  },\n",
    "  \"configs12\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps012ter50\",\n",
    "    \"eps\": 0.12,\n",
    "    \"max_iter\": 50\n",
    "  },\n",
    "  \"configs13\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps024iter50\",\n",
    "    \"eps\": 0.24,\n",
    "    \"max_iter\": 50\n",
    "  },\n",
    "  \"configs14\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps048iter50\",\n",
    "    \"eps\": 0.48,\n",
    "    \"max_iter\": 50\n",
    "  },\n",
    "  \"configs15\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps003iter100\",\n",
    "    \"eps\": 0.03,\n",
    "    \"max_iter\": 100\n",
    "  },\n",
    "  \"configs16\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps006iter100\",\n",
    "    \"eps\": 0.06,\n",
    "    \"max_iter\": 100\n",
    "  },\n",
    "  \"configs17\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps0121ter100\",\n",
    "    \"eps\": 0.12,\n",
    "    \"max_iter\": 100\n",
    "  },\n",
    "  \"configs18\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps024iter100\",\n",
    "    \"eps\": 0.24,\n",
    "    \"max_iter\": 100\n",
    "  },\n",
    "  \"configs19\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps048iter100\",\n",
    "    \"eps\": 0.48,\n",
    "    \"max_iter\": 100\n",
    "  }\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Examples\n",
    "It was previously found that the magnitude of epsilon is directly proportional to the classifier's error rate and the error rate of the three models.\n",
    "\n",
    "Now, it is possible to draw the correlation in BIM's number of iterations and error rate.\n",
    " \n",
    "**Images at Various Epsilons (50 Iterations)\n",
    "\n",
    "When the value of epsilon increases, the recognizability of the image decreases; but, the error rate of the classifier increases. See the table of images below. \n",
    "\n",
    "0.03            |  0.06 | 0.12 | 0.24 | 0.48\n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|\n",
    "![](images/bim_eps003_iter50.png)  |  ![](images/bim_eps006_iter50.png) |  ![](images/bim_eps012_iter50.png) |  ![](images/bim_eps024_iter50.png) |  ![](images/bim_eps048_iter50.png)\n",
    "\n",
    "\n",
    "**Images at Various Epsilons (100 Iterations)**\n",
    " \n",
    "Interestingly enough, doubling the iterations at the same epsilon does not double the error rate, and sometimes it does not increase the error rate a significant amouunt.\n",
    "\n",
    "\n",
    "0.03            |  0.06 | 0.12 | 0.24 | 0.48\n",
    ":-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|\n",
    "![](images/bim_eps003_iter100.png)  |  ![](images/bim_eps006_iter100.png) |  ![](images/bim_eps012_iter100.png) |  ![](images/bim_eps024_iter100.png) |  ![](images/bim_eps048_iter100.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As seen from the above images, there was not a human-noticeable difference between images manipulated through BIM at 50 iterations and images manipulated by BIM at 100 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Evaluated Models\n",
    "| BIM at 50 Iterations |            |            |            |   | BIM at 100 Iterations |            |            |             |\n",
    "|----------------------|------------|------------|------------|---|-----------------------|------------|------------|-------------|\n",
    "| Adversarial Examples | UM         | Ensemble   | PGD-ADT    |   | Adversarial Examples  | UM         | Ensemble   | PGD-ADT     |\n",
    "| BIM_eps_0.03         | 0.05247225 | 0.00201816 | 0.00605449 |   | BIM_eps_0.03          | 0.05348133 | 0.00201816 |  0.00605449 |\n",
    "| BIM_eps_0.06         | 0.30575177 | 0.00302725 | 0.01311806 |   | BIM_eps_0.06          |  0.3148335 | 0.00302725 | 0.013118063 |\n",
    "| BIM_eps_0.12         | 0.98486377 | 0.01009082 | 0.02825429 |   | BIM_eps_0.12          | 0.98789102 | 0.01009082 |  0.02926337 |\n",
    "| BIM_eps_0.24         | 0.99091826 | 0.07568113 |  0.1654894 |   | BIM_eps_0.24          | 0.99091826 | 0.08072654 | 0.166498486 |\n",
    "| BIM_eps_0.48         | 0.99091826 | 0.65489405 | 0.95358224 |   | BIM_eps_0.48          | 0.99091826 | 0.65993946 | 0.956609485 |\n",
    "\n",
    "In FGSM and PGD, it was possible to see that the magnitude of epsilon directly correlated to a higher error rate. In this iterative approach, we're able to see a correlation between the number of iterations, or individual attacks, and error rate. Iteratively and independantly attacking the models with BIM seemed to cause the highest error rates when compared to the other attack methodologies. It was also the **most expensive computationally** (time of execution) due to its iterative nature. \n",
    "\n",
    "An interesting point is that while there was an increase in error across the models between 50 iteration experiments and 100 iteration experiments, there was not a significant benefit in running BIM at 100 iterations.\n",
    "\n",
    "This information is also visiable in the charts below.\n",
    "\n",
    "50 Iterations            |  100 Iterations\n",
    ":-------------------------:|:--------------------------:\n",
    "![](images/BIM_50_Iterations_Evaluations.png)  |  ![](images/BIM_100_Iterations_Evaluations.png)\n",
    "\n",
    "\n",
    "\n",
    "# Conclusion \n",
    "\n",
    "After generating 20 adversarial examples and evaluating them against Vanilla Athena, an Undefended Model, and PGD-ADT, the results favor Vanilla Athena with the chosen ensemble of 5 defenses. It is interesting to note that no matter what the attack method was, Athena has a significant advantage over the other models, especially when the image was still in a state that looked mostly recoverable and recognizable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Contributions\n",
    "### Austin Staton\n",
    "Worked Primarily on the report, as well as developed AEs for the PGD attack method with Daniel Jones.\n",
    "### Daniel Jones\n",
    "Worked on developing AEs for the FGSM and BIM attack methods with Praful Chunchu and Ravi Patel. Also worked on data aggregation and some repository maintenance.\n",
    "### Praful Chunchu\n",
    "Worked on developing AEs for FGSM and BIM attack methods with Ravi Patel and Daniel Jones.\n",
    "### Ravi Patel\n",
    "Worked on developing AEs for FGSM and BIM attack methods with Praful Chunchu and Daniel Jones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
