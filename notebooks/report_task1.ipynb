{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning ystems Task 1\n",
    "_Daniel Jones, Praful Chunchu, Ravi Patel and Austin Staton_\n",
    "\n",
    "**Objective**: Generating adversarial attacks in the context of a zero-knowledge threat model.\n",
    "\n",
    "We will be exploring various adversarial attacks, including: _Projected Gradient Descent_, _Fast Gradient Sign Method_, and the _Basic Iterative Method_. We will be generating these adversarial examples with a specific set of tuned parameters for each attack, to also allow for a demonstration of the effectiveness of different machine learning models.\n",
    " \n",
    "### Experimental Design\n",
    "We will be attacking three different models: an undefended model, the [vanilla Athena](https://github.com/softsys4ai/athena), and PGD-ADT. \n",
    "\n",
    "In order to effectively determine the differences in success (or rather, the differences in errors) between each different approach, identical parameters will be sent to each different model, by adversarial attack. So, for any _one_ attack, (PGD, FGSM, BIM) the parameters testing the attack's efficacy will remain constistent across the three differently independant models. This does not mean that, for example, the values of inputs to attack type remains consistent for all different models. \n",
    "\n",
    "We expect this to give some experimental consistency to our results.\n",
    "\n",
    "\n",
    "#### Subsampling\n",
    "To increase execution speed, we opted to generate subsamples of data using the provided `subsample.py` script provided.This sctipt generated a subsample at a ratio of `0.1` and the subsamples/sublabels used for this experiment can be found in the `/results` folder. Our generated adversarial examples are also located within this folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Projected Gradient Descent (PGD)\n",
    "PGD attacks are white-box attacks, specifically designed to take advantage of each layer's weight in the ML model. This attack has a parameter, `epsilon`, that attempts to find the biggest weaknesses in the model, while trying to minimize the input distortion or alteration. We exectued the PGD attack with five different values of `epsilon`.\n",
    "\n",
    "#### The Inputs\n",
    "The parameters of epsilon for the attacks are `0.03`, `0.07`, `0.09`, `0.12`, and `0.18`. When we increase epsilon, two things will happen. The first, is that the inputs (images) will be increasingly poised to take advantage of the model's weights. The second, which occurs as an effect of the first, is that the image is increasingly distorted. This is a form of constrained optimization problem that would need to be tuned to each attack's purpose. \n",
    "\n",
    "As an example, if one was attempting to bypass the content filtering of an image upload service, the image would need to be _mostly_ recoverable. Bypassing a content filter to upload an unrecognizable image would not make much sense in practical applications.\n",
    "\n",
    "The inputs, in JSON form, looked like the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_attacks': 5,\n",
       " 'configs0': {'attack': 'pgd', 'description': 'PGD_eps0.03', 'eps': 0.03},\n",
       " 'configs1': {'attack': 'pgd', 'description': 'PGD_eps0.07', 'eps': 0.07},\n",
       " 'configs2': {'attack': 'pgd', 'description': 'PGD_eps0.09', 'eps': 0.09},\n",
       " 'configs3': {'attack': 'pgd', 'description': 'PGD_eps0.12', 'eps': 0.12},\n",
       " 'configs4': {'attack': 'pgd', 'description': 'PGD_eps0.18', 'eps': 0.18}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"num_attacks\": 5,\n",
    "  \"configs0\": {\n",
    "    \"attack\": \"pgd\",\n",
    "    \"description\": \"PGD_eps0.03\",\n",
    "    \"eps\": 0.03\n",
    "  },\n",
    "  \"configs1\": {\n",
    "    \"attack\": \"pgd\",\n",
    "    \"description\": \"PGD_eps0.07\",\n",
    "    \"eps\": 0.07\n",
    "  },\n",
    "  \"configs2\": {\n",
    "    \"attack\": \"pgd\",\n",
    "    \"description\": \"PGD_eps0.09\",\n",
    "    \"eps\": 0.09\n",
    "  },\n",
    "  \"configs3\": {\n",
    "    \"attack\": \"pgd\",\n",
    "    \"description\": \"PGD_eps0.12\",\n",
    "    \"eps\": 0.12\n",
    "  },\n",
    "  \"configs4\": {\n",
    "    \"attack\": \"pgd\",\n",
    "    \"description\": \"PGD_eps0.18\",\n",
    "    \"eps\": 0.18\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Examples\n",
    "The results matched our hypothsis. Meaning, as the value of the tuned parameter `epsilon` increased, more distortion was created in the image, more model weights were exploited, and more errors occured.\n",
    "\n",
    "\n",
    "**Error Rates**\n",
    " * `epsilson:0.03` -> `error_rate: 0.052` (5.2%)\n",
    " * `epsilson:0.07` -> `error_rate: 0.306` (30.6%)\n",
    " * `epsilson:0.09` -> `error_rate: 0.563` (56.3%)\n",
    " * `epsilson:0.12` -> `error_rate: 0.855` (85.5%)\n",
    " * `epsilson:0.18` -> `error_rate: 1.0` (100%)\n",
    " \n",
    " \n",
    "**One Generated Image at Each Epsilion Value**\n",
    "As you can see below, as the value of epsilon increases, the recognizability of the image decreases; but, the error rate of the classifier increases.\n",
    " \n",
    "In the title of each image, the `X->Y` denotes the classifiers interpretation of the number. `X` represents the original value of the image; `Y` represents its classification by the model after pertubation.\n",
    "\n",
    "![Epsilon 0.03 Error](img/pgd_eps003_error.png)\n",
    "![Epsilon 0.07 Error](img/pgd_eps007_error.png)\n",
    "![Epsilon 0.09 Error](img/pgd_eps009_error.png)\n",
    "![Epsilon 0.12 Error](img/pgd_eps012_error.png)\n",
    "![Epsilon 0.18 Error](img/pgd_eps018_error.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Evaluated Models\n",
    "| **Epsilon** | **Undefended Model** | **Ensemble of WDs** | **PGD-ADT**|\n",
    "|:---------:|:------------:|:---------:|:------------:|\n",
    "| 0.03 | 0.04137235116044399 | 0.0030272452068617556 | 0.006054490413723511 |\n",
    "| 0.07 |0.2956609485368315|0.007063572149344097|0.015136226034308779|\n",
    "| 0.09 |0.5539858728557013|0.011099899091826439|0.017154389505549948|\n",
    "| 0.12 |0.8466195761856711|0.017154389505549948|0.029263370332996974|\n",
    "| 0.18 |0.9868819374369324|0.04944500504540868|0.055499495459132187|\n",
    "\n",
    "We can see in this table, that as the value of epsilon increased, the error rate increased as well. This makes sense, since epsilon represents a maginitude of damage to the image. With enough pertubation, one's own mind begin to misclassify an image. It would be expected of a classifier misclassify it too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Fast Gradient Signed Method (FGSM)\n",
    "FGSM adversarial attacks are white-box attacks that exploit the gradients, or parameters, to a neural network. It is designed to prioritize speed, rather than designed around solving the constrained optimization problem between data integrity and perturbation, like PGD.\n",
    "\n",
    "FGSM uses the sign of loss function (what is somewhat similar to the linear \"direction\" to the next classification) to determine where the model could easiest misrepresent the data, moves in a \"distance\" of `epsilon` to that next space within the network. \n",
    "\n",
    "With this vector, having a direction (the sign of a loss function) and magnitude (epsilon), can be used to alter input to fool a classifier. \n",
    "\n",
    "#### The Inputs\n",
    "The parameters of epsilon (i.e., distance) for the FGSM  attacks are: `0.1`, `0.5`, `0.7`, `0.8`, and `0.9`. In FGSM, `epsilon` is a scalar value that determines how much pertubation to create in the classification.\n",
    "\n",
    "The inputs, in JSON format, looked like the below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_attacks': 5,\n",
       " 'configs0': {'attack': 'fgsm', 'description': 'fgsm_eps0.1', 'eps': 0.1},\n",
       " 'configs1': {'attack': 'fgsm', 'description': 'fgsm_eps0.5', 'eps': 0.5},\n",
       " 'configs2': {'attack': 'fgsm', 'description': 'fgsm_eps0.7', 'eps': 0.7},\n",
       " 'configs3': {'attack': 'fgsm', 'description': 'fgsm_eps0.8', 'eps': 0.8},\n",
       " 'configs4': {'attack': 'fgsm', 'description': 'fgsm_eps0.9', 'eps': 0.9}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"num_attacks\": 5,\n",
    "  \"configs0\": {\n",
    "    \"attack\": \"fgsm\",\n",
    "    \"description\": \"fgsm_eps0.1\",\n",
    "    \"eps\": 0.1\n",
    "  },\n",
    "  \"configs1\": {\n",
    "    \"attack\": \"fgsm\",\n",
    "    \"description\": \"fgsm_eps0.5\",\n",
    "    \"eps\": 0.5\n",
    "  },\n",
    "  \"configs2\": {\n",
    "    \"attack\": \"fgsm\",\n",
    "    \"description\": \"fgsm_eps0.7\",\n",
    "    \"eps\": 0.7\n",
    "  },\n",
    "  \"configs3\": {\n",
    "    \"attack\": \"fgsm\",\n",
    "    \"description\": \"fgsm_eps0.8\",\n",
    "    \"eps\": 0.8\n",
    "  },\n",
    "  \"configs4\": {\n",
    "    \"attack\": \"fgsm\",\n",
    "    \"description\": \"fgsm_eps0.9\",\n",
    "    \"eps\": 0.9\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Examples\n",
    "The results matched our hypothsis. Meaning, as the value of the tuned parameter `epsilon` increased, the 'distance' away from the original classification changed.\n",
    "\n",
    "**Error Rates**\n",
    " * `epsilson:0.1` -> `error_rate: 0.273` (27.3%)\n",
    " * `epsilson:0.5` -> `error_rate: 0.904` (90.4%)\n",
    " * `epsilson:0.7` -> `error_rate: 0.906` (90.6%)\n",
    " * `epsilson:0.8` -> `error_rate: 0.917` (91.7%)\n",
    " * `epsilson:0.9` -> `error_rate: 0.908` (90.8%)\n",
    " \n",
    " \n",
    "**One Generated Image at Each Epsilion Value**\n",
    "\n",
    "As you can see below, as the value of epsilon increases, the recognizability of the image decreases; but, the error rate of the classifier increases.\n",
    "\n",
    "In the title of each image, the `X->Y` denotes the classifiers interpretation of the number. \"X\" represents the original value of the image; \"Y\" represents its classification by the model after pertubation.\n",
    "\n",
    "![Epsilon 0.1 Error](img/fgsm_eps01.png)\n",
    "![Epsilon 0.5 Error](img/fgsm_eps0.5.png)\n",
    "![Epsilon 0.7 Error](img/fgsm_eps07.png)\n",
    "![Epsilon 0.8 Error](img/fgsm_eps08.png)\n",
    "![Epsilon 0.9 Error](img/fgsm_eps09.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Evaluated Models\n",
    "|**Epsilon**|**Undefended Model**|**Ensemble of WDs** |**PGD-ADT**|\n",
    "|:---------:|:------------:|:---------:|:------------:|\n",
    "| 0.1 |0.2633703329969727|0.0121089808274470|0.0221997981836528|\n",
    "| 0.5 |0.8980827447023209|0.8940464177598385|0.8728557013118062|\n",
    "| 0.7 |0.8990918264379415|0.8940464177598385|0.8809283551967709|\n",
    "| 0.8 |0.9091826437941474|0.8940464177598385|0.8758829465186687|\n",
    "| 0.9 |0.9011099899091827|0.8940464177598385|0.8859737638748738|\n",
    "\n",
    "In the case of the undefended model, there was a general direct proportionality between epsilon and the error rate of the model. In Vanilla Athena (the ensemble of weak defenses) and PGD-ADT, we did not see this trend. This could be attributed to the framework being trained to defend against this adversarial attack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Basic Iteractive Method (BIM)\n",
    "BIM attacks are a variant to FGSM attacks. The same direction is computed from the loss function and magnitude is found with epsilon; but, the adversarial attack is performed many different times, 'iteratively', in increasing step sizes. \n",
    "\n",
    "#### The Inputs\n",
    "The parameters of epsilon for the attacks are `0.9`, `0.4`, `0.01`, `0.9`, and `0.6`. Respectively, the number of iterations to reach these epsilon values were performed at `100`, `75`, `100`, `40`, and, `25`.\n",
    "\n",
    "The inputs, in JSON form, looked like the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_attacks': 5,\n",
       " 'configs0': {'attack': 'bim',\n",
       "  'description': 'bim_eps0.9iter100',\n",
       "  'eps': 0.9,\n",
       "  'max_iter': 100},\n",
       " 'configs1': {'attack': 'bim',\n",
       "  'description': 'bim_eps0.4iter75',\n",
       "  'eps': 0.4,\n",
       "  'max_iter': 75},\n",
       " 'configs2': {'attack': 'bim',\n",
       "  'description': 'bim_eps0.01ter100',\n",
       "  'eps': 0.01,\n",
       "  'max_iter': 100},\n",
       " 'configs3': {'attack': 'bim',\n",
       "  'description': 'bim_eps0.9iter40',\n",
       "  'eps': 0.9,\n",
       "  'max_iter': 40},\n",
       " 'configs4': {'attack': 'bim',\n",
       "  'description': 'bim_eps0.6iter25',\n",
       "  'eps': 0.6,\n",
       "  'max_iter': 25}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"num_attacks\": 5,\n",
    "  \"configs0\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps0.9iter100\",\n",
    "    \"eps\": 0.9,\n",
    "    \"max_iter\": 100\n",
    "  },\n",
    "  \"configs1\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps0.4iter75\",\n",
    "    \"eps\": 0.4,\n",
    "    \"max_iter\": 75\n",
    "  },\n",
    "  \"configs2\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps0.01ter100\",\n",
    "    \"eps\": 0.01,\n",
    "    \"max_iter\": 100\n",
    "  },\n",
    "  \"configs3\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps0.9iter40\",\n",
    "    \"eps\": 0.9,\n",
    "    \"max_iter\": 40\n",
    "  },\n",
    "  \"configs4\": {\n",
    "    \"attack\": \"bim\",\n",
    "    \"description\": \"bim_eps0.6iter25\",\n",
    "    \"eps\": 0.6,\n",
    "    \"max_iter\": 25\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Examples\n",
    "We have previously found that the magnitude of epsilon is directly proportional to the classifier's error rate and the error rate of the three models.\n",
    "\n",
    "Now, we are able to draw the correlation in BIM's number of iterations and error rate.\n",
    "\n",
    "**Error Rates**\n",
    " * `epsilson:0.9; max_iter: 100` -> `error_rate: 1.0` (100%)\n",
    " * `epsilson:0.4; max_iter: 75` -> `error_rate: 1.0` (100%)\n",
    " * `epsilson:0.01; max_iter: 100` -> `error_rate: 0.018` (1.8%)\n",
    " * `epsilson:0.9; max_iter: 40` -> `error_rate: 1.0` (100%)\n",
    " * `epsilson:0.6; max_iter: 25` -> `error_rate: 1.0` (100%)\n",
    " \n",
    " \n",
    " **One Generated Image at Each Epsilion Value**\n",
    "As you can see below, as the value of epsilon increases, the recognizability of the image decreases; but, the error rate of the classifier increases.\n",
    "\n",
    "![Epsilon 0.9 Iter 100 Error](img/bim_eps0.9iter100.png)\n",
    "![Epsilon 0.4 Iter 75 Error](img/bim_eps0.4iter75.png)\n",
    "![Epsilon 0.01 Iter 100 Error](img/bim_eps0.01iter100.png)\n",
    "![Epsilon 0.9 Iter 40 Error](img/bim_eps0.9iter40.png)\n",
    "![Epsilon 0.6 Iter 25 Error](img/bim_eps0.6iter25.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Evaluated Models\n",
    "|**Epsilon**|**Iterations**|**Undefended**|**Vanilla Athena**|**PGD-ADT**|\n",
    "|:-----:|:------:|:------:|:-----:|:-----:|\n",
    "|0.9|100|0.9909182643794148|0.9112008072653885|0.9909182643794148|\n",
    "|0.4|75|0.9909182643794148|0.8708375378405651|0.8092835519677094|\n",
    "|0.01|100|0.009081735620585268|0.0030272452068617556|0.005045408678102927|\n",
    "|0.9|40|0.9909182643794148|0.9112008072653885|0.9909182643794148|\n",
    "|0.6|25|0.9909182643794148|0.9132189707366297|0.9808274470232089|\n",
    "\n",
    "In FGSM and PGD, we were able to see that the magnitude of epsilon directly correlated to a higher error rate. In this iterative approach, we're able to see a correlation between the number of iterations, or individual attacks, and error rate. Iteratively and independantly attacking the models with BIM seemed to cause the highest error rates when compared to the other attack methodologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
